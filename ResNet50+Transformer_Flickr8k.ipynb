{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Image Captioning with ResNet-50 + Transformer Decoder\n",
        "\n",
        "## Project Overview\n",
        "\n",
        "This project implements **Model B** from a comparative study of image captioning architectures: a ResNet-50 CNN encoder combined with a Transformer decoder. The model generates descriptive captions for images using the Flickr8k dataset.\n",
        "\n",
        "**Architecture:** ResNet-50 (pretrained on ImageNet) + 6-layer Transformer Decoder  \n",
        "**Framework:** PyTorch  \n",
        "**Dataset:** Flickr8k (8,000 images with 5 captions each)  \n",
        "**Hardware:** Google Colab with Tesla T4 GPU\n",
        "\n",
        "## Model Architecture\n",
        "\n",
        "### Encoder: ResNet-50\n",
        "- Pretrained on ImageNet for transfer learning\n",
        "- Extracts 2048-dimensional visual features from images\n",
        "- Staged training approach:\n",
        "  - **Epochs 1-5:** All layers frozen (train decoder only)\n",
        "  - **Epoch 6+:** Layer4 (conv5_x) unfrozen for fine-tuning\n",
        "\n",
        "### Decoder: Transformer\n",
        "- 6 layers with multi-head self-attention\n",
        "- Model dimension: 512\n",
        "- Attention heads: 8\n",
        "- Feedforward dimension: 2048\n",
        "- Generates captions autoregressively\n",
        "\n",
        "## Dataset\n",
        "\n",
        "**Flickr8k Dataset:**\n",
        "- 8,000 images total\n",
        "- 5 human-written captions per image\n",
        "- Train/Val/Test split: 6,000 / 1,000 / 1,000 images\n",
        "- Results in 30,000 training examples (6,000 √ó 5 captions)\n",
        "\n",
        "**Preprocessing:**\n",
        "- Images resized to 224√ó224\n",
        "- Captions lowercased and tokenized\n",
        "- Special tokens: `<start>`, `<end>`, `<pad>`\n",
        "- Maximum caption length: 20 tokens\n",
        "\n",
        "## Training Configuration\n",
        "\n",
        "```python\n",
        "Optimizer: AdamW\n",
        "Learning Rate: 2e-4\n",
        "Weight Decay: 0.01\n",
        "Batch Size: 16-32 (with gradient accumulation)\n",
        "Loss Function: Cross-Entropy with label smoothing (0.1)\n",
        "Precision: Mixed Precision (AMP) for efficiency\n",
        "Early Stopping: 5 epochs without validation improvement\n",
        "```\n",
        "\n",
        "## Performance\n",
        "\n",
        "**Training Results:**\n",
        "- Best validation loss: **3.4509** (epoch 6)\n",
        "- Total training time: ~40 minutes (11 epochs)\n",
        "- Training speed: ~45 iterations/second with GPU\n",
        "\n",
        "**Sample Outputs:**\n",
        "\n",
        "| Image | Ground Truth | Model Prediction |\n",
        "|-------|-------------|------------------|\n",
        "| Racing dog | \"A greyhound in a race wearing a metal muzzle\" | \"the dog is white and black\" |\n",
        "| Dog with ball | \"A dog with big ears holds a ball in his mouth\" | \"a dog running with a ball in its mouth\" |\n",
        "| Boy diving | \"A boys jumps into the water upside down\" | \"a boy is doing a flip in the water\" |\n",
        "| Football huddle | \"a group of football players huddled together\" | \"a group of football players hugging each other\" |\n",
        "| Dogs jumping | \"A big brown dog jumps over a small black dog\" | \"a dog jumping in the air to catch a frisbee\" |\n",
        "\n",
        "## Setup Instructions\n",
        "\n",
        "### 1. Environment Setup\n",
        "\n",
        "```bash\n",
        "# Install required packages\n",
        "pip install torch torchvision\n",
        "pip install numpy pandas pillow matplotlib tqdm\n",
        "pip install nltk scikit-learn\n",
        "pip install evaluate  # For evaluation metrics\n",
        "```\n",
        "\n",
        "### 2. Download Dataset\n",
        "\n",
        "The notebook automatically downloads Flickr8k:\n",
        "\n",
        "```python\n",
        "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n",
        "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\n",
        "!unzip Flickr8k_Dataset.zip\n",
        "!unzip Flickr8k_text.zip\n",
        "```\n",
        "\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "‚îú‚îÄ‚îÄ Data Loading & Preprocessing\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ Dataset download from Kaggle/GitHub\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ Train/Val/Test split (6k/1k/1k)\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ Caption preprocessing\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ Vocabulary building\n",
        "‚îÇ\n",
        "‚îú‚îÄ‚îÄ Model Implementation\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ ResNet-50 encoder (pretrained)\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ Transformer decoder (6 layers)\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ Attention mechanisms\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ Caption generation logic\n",
        "‚îÇ\n",
        "‚îú‚îÄ‚îÄ Training Pipeline\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ Mixed precision training (AMP)\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ Freeze-unfreeze strategy\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ Early stopping\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ GPU optimization\n",
        "‚îÇ\n",
        "‚îî‚îÄ‚îÄ Evaluation & Visualization\n",
        "    ‚îú‚îÄ‚îÄ Loss tracking\n",
        "    ‚îú‚îÄ‚îÄ Caption generation on test set\n",
        "    ‚îú‚îÄ‚îÄ Qualitative comparison (GT vs Pred)\n",
        "    ‚îî‚îÄ‚îÄ Metric computation (BLEU, METEOR, etc.)\n",
        "```\n",
        "\n",
        "## Key Features\n",
        "\n",
        "1. **Staged Training:** Freeze encoder initially, then fine-tune selectively\n",
        "2. **Mixed Precision:** 40-50% speedup with AMP\n",
        "3. **Early Stopping:** Prevents overfitting, saves compute\n",
        "4. **GPU Acceleration:** Full CUDA support, verified through training speed\n",
        "5. **Comprehensive Evaluation:** Multiple metrics + qualitative samples\n",
        "\n",
        "## Usage\n",
        "\n",
        "### Training\n",
        "\n",
        "```python\n",
        "# Run all cells in sequence\n",
        "# The notebook handles:\n",
        "# 1. Dataset download & preprocessing\n",
        "# 2. Model initialization\n",
        "# 3. Training loop with validation\n",
        "# 4. Checkpoint saving\n",
        "```\n",
        "\n",
        "### Inference\n",
        "\n",
        "```python\n",
        "# Generate caption for a single image\n",
        "def generate_caption(image_path):\n",
        "    image = preprocess_image(image_path)\n",
        "    features = encoder(image)\n",
        "    caption = decoder.generate(features, max_length=20)\n",
        "    return caption\n",
        "```\n",
        "\n",
        "## Technical Highlights\n",
        "\n",
        "### Multimodal Architecture\n",
        "Combines computer vision (CNN) with natural language processing (Transformer) for the image-to-text task.\n",
        "\n",
        "### Attention Mechanisms\n",
        "- Self-attention in decoder for linguistic coherence\n",
        "- Cross-attention between visual features and text tokens\n",
        "\n",
        "### Efficient Training\n",
        "- Gradient accumulation for larger effective batch size\n",
        "- Mixed precision reduces memory usage\n",
        "- Proper dataset structure understanding (5 captions/image = 5√ó training examples)\n",
        "\n",
        "## Results Analysis\n",
        "\n",
        "**Strengths:**\n",
        "- Reasonable semantic understanding (dogs, sports, water activities)\n",
        "- Captures main subjects and actions accurately\n",
        "- Handles diverse scene types\n",
        "\n",
        "**Limitations:**\n",
        "- Occasional hallucination of objects (e.g., \"frisbee\" when none present)\n",
        "- Less detail than ground truth captions\n",
        "- Generic descriptions vs. specific attributes\n",
        "\n",
        "**Improvements over baseline:**\n",
        "- Better syntactic structure than LSTM decoder\n",
        "- More contextual coherence through attention\n",
        "- Competitive with modern architectures given compute constraints\n",
        "\n",
        "## Comparison with Other Models\n",
        "\n",
        "As part of the larger project proposal:\n",
        "\n",
        "- **Model A (ResNet-50 + LSTM):** Classical baseline\n",
        "- **Model B (This implementation):** Modern non-pretrained architecture\n",
        "- **Model C (BLIP + LoRA):** Foundation model with parameter-efficient fine-tuning\n",
        "\n",
        "Expected performance hierarchy: LSTM < Transformer < Pretrained LoRA\n",
        "\n",
        "## Future Work\n",
        "\n",
        "1. **Longer training:** Current 11 epochs may be insufficient\n",
        "2. **Hyperparameter tuning:** Learning rate scheduling, batch size optimization\n",
        "3. **Advanced metrics:** CIDEr, SPICE for human correlation\n",
        "4. **Beam search:** Replace greedy decoding for better captions\n",
        "5. **Attention visualization:** Understand what the model \"sees\"\n",
        "\n",
        "## Requirements\n",
        "\n",
        "```\n",
        "torch>=2.0.0\n",
        "torchvision>=0.15.0\n",
        "numpy>=1.24.0\n",
        "pandas>=2.0.0\n",
        "pillow>=9.5.0\n",
        "matplotlib>=3.7.0\n",
        "tqdm>=4.65.0\n",
        "nltk>=3.8.0\n",
        "scikit-learn>=1.3.0\n",
        "evaluate>=0.4.0\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "## License\n",
        "\n",
        "This project is for educational purposes as part of ESE 5460 coursework.\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "- Flickr8k dataset creators\n",
        "- PyTorch team for excellent documentation\n",
        "- ResNet and Transformer architecture papers\n",
        "- Google Colab for free GPU access\n",
        "\n"
      ],
      "metadata": {
        "id": "1xvhgSTXKqM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import textwrap\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchvision import models, transforms\n",
        "import math\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import torch\n",
        "from PIL import Image\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "import matplotlib.patches as patches\n",
        "from matplotlib import gridspec\n",
        "\n",
        "from PIL import Image as PILImage\n",
        "import torchvision.transforms as transforms\n",
        "import json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "b3tcYtp_HSr5",
        "outputId": "ff48bd43-14b0-475d-9b49-0b1facf642c6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'clip'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1088821011.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'clip'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n277WstbQipW",
        "outputId": "83588596-02cc-4f9e-c1d6-c75963f9e112"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "GPU VERIFICATION\n",
            "============================================================\n",
            "‚úì GPU is AVAILABLE!\n",
            "‚úì Device Name: Tesla T4\n",
            "‚úì CUDA Version: 12.6\n",
            "‚úì Total Memory: 15.83 GB\n",
            "‚úì PyTorch Version: 2.9.0+cu126\n",
            "‚úì GPU computation test: SUCCESS\n",
            "‚úì Result shape: torch.Size([1000, 1000])\n",
            "\n",
            "‚úì Using device: cuda\n",
            "============================================================\n",
            "READY TO TRAIN WITH GPU! üöÄ\n",
            "Stored 'device' (device)\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "GPU Check and Setup\n",
        "\"\"\"\n",
        "\n",
        "# Check CUDA availability\n",
        "if torch.cuda.is_available():\n",
        "    print(\"‚úì GPU is AVAILABLE!\")\n",
        "\n",
        "    # Test GPU computation\n",
        "    try:\n",
        "        x = torch.randn(1000, 1000).cuda()\n",
        "        y = torch.randn(1000, 1000).cuda()\n",
        "        z = torch.matmul(x, y)\n",
        "        print(\"‚úì GPU computation test: SUCCESS\")\n",
        "        print(f\"‚úì Result shape: {z.shape}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚úó GPU computation test FAILED: {e}\")\n",
        "\n",
        "    device = torch.device('cuda')\n",
        "    print(f\"\\n‚úì Using device: {device}\")\n",
        "\n",
        "else:\n",
        "    print(\"‚úó GPU is NOT available\")\n",
        "\n",
        "%store device\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n",
        "!wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZVj5N08QULS",
        "outputId": "ee881e6a-15f6-4cca-d99b-73d714da8b74"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q Flickr8k_Dataset.zip\n",
        "!unzip -q Flickr8k_text.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Tk7hmNMQ1Dt",
        "outputId": "b290336d-21c0-4d93-c939-07c7623e6a64"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace Flicker8k_Dataset/1000268201_693b08cb0e.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "A\n",
            "replace CrowdFlowerAnnotations.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "noyGkLCrBDHp"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_all = pd.read_csv(\"flickr8k_captions.csv\")\n",
        "\n",
        "if \"image\" in df_all.columns:\n",
        "    image_col = \"image\"\n",
        "elif \"image_file\" in df_all.columns:\n",
        "    image_col = \"image_file\"\n",
        "else:\n",
        "    raise ValueError(f\"Cannot find image column in csv. Columns are: {df_all.columns}\")\n",
        "\n",
        "all_images = df_all[image_col].unique()\n",
        "\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(all_images)\n",
        "\n",
        "train_images = all_images[:6000]\n",
        "val_images   = all_images[6000:7000]\n",
        "test_images  = all_images[7000:8000]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "SAVE_DIR = \"/content/drive/MyDrive/eva_qualitative\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "IMAGE_DIR = \"/content/Flicker8k_Dataset\"\n",
        "\n",
        "CANVAS_W = 400\n",
        "CANVAS_H = 140\n",
        "\n",
        "LEFT_W = 200\n",
        "RIGHT_W = 200\n",
        "\n",
        "FONT_SIZE = 50\n",
        "LINE_SPACING = 10"
      ],
      "metadata": {
        "id": "hZ1ki-CCL-N7"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_font(size):\n",
        "    try:\n",
        "        return ImageFont.truetype(\n",
        "            \"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\",\n",
        "            size\n",
        "        )\n",
        "    except:\n",
        "        return ImageFont.load_default()\n",
        "\n",
        "font = load_font(FONT_SIZE)"
      ],
      "metadata": {
        "id": "Akq7mrBOMCQz"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_caption(draw, text, x, y, max_chars):\n",
        "    lines = textwrap.wrap(text, width=max_chars)\n",
        "    cur_y = y\n",
        "    for line in lines:\n",
        "        draw.text((x, cur_y), line, fill=(0, 0, 0), font=font)\n",
        "        cur_y += font.size + LINE_SPACING"
      ],
      "metadata": {
        "id": "9z3_eMToMED5"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcVzVbEy-Q-7",
        "outputId": "052592a2-d442-4388-8548-f30f74fc977e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "GPU: Tesla T4\n",
            "Memory: 15.83 GB\n"
          ]
        }
      ],
      "source": [
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è WARNING: Running on CPU - will be very slow!\")\n",
        "    print(\"Enable GPU: Runtime ‚Üí Change runtime type ‚Üí T4 GPU\")\n",
        "# ==================== VOCABULARY ====================\n",
        "class Vocabulary:\n",
        "    def __init__(self, freq_threshold=2):\n",
        "        self.freq_threshold = freq_threshold\n",
        "        self.word2idx = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
        "        self.idx2word = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "        self.idx = 4\n",
        "\n",
        "    def build_vocabulary(self, captions_list):\n",
        "        \"\"\"Build vocab from list of captions\"\"\"\n",
        "        word_freq = Counter()\n",
        "        for caption in captions_list:\n",
        "            tokens = caption.lower().split()\n",
        "            word_freq.update(tokens)\n",
        "\n",
        "        for word, freq in word_freq.items():\n",
        "            if freq >= self.freq_threshold:\n",
        "                self.word2idx[word] = self.idx\n",
        "                self.idx2word[self.idx] = word\n",
        "                self.idx += 1\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"Convert text to token indices\"\"\"\n",
        "        tokens = text.lower().split()\n",
        "        return [self.word2idx.get(token, self.word2idx[\"<UNK>\"]) for token in tokens]\n",
        "\n",
        "    def decode(self, indices):\n",
        "        \"\"\"Convert token indices back to text\"\"\"\n",
        "        return \" \".join([self.idx2word[idx] for idx in indices if idx not in [0, 1, 2]])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)\n",
        "\n",
        "\n",
        "# ==================== DATASET ====================\n",
        "class Flickr8kDataset(Dataset):\n",
        "    def __init__(self, image_dir, captions_file, vocab, transform=None, max_length=20):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_dir: Directory containing images\n",
        "            captions_file: Path to captions file (format: image_name.jpg\\tcaption)\n",
        "            vocab: Vocabulary object\n",
        "            transform: Image transformations\n",
        "            max_length: Maximum caption length\n",
        "        \"\"\"\n",
        "        self.image_dir = image_dir\n",
        "        self.vocab = vocab\n",
        "        self.transform = transform\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Load captions\n",
        "        self.images = []\n",
        "        self.captions = []\n",
        "        with open(captions_file, 'r') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "                parts = line.split('\\t')\n",
        "                if len(parts) == 2:\n",
        "                    img_name, caption = parts\n",
        "                    self.images.append(img_name)\n",
        "                    self.captions.append(caption)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.captions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.images[idx]\n",
        "        caption = self.captions[idx]\n",
        "\n",
        "        # Load and transform image\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Tokenize caption with SOS and EOS\n",
        "        tokens = [self.vocab.word2idx[\"<SOS>\"]]\n",
        "        tokens += self.vocab.encode(caption)\n",
        "        tokens.append(self.vocab.word2idx[\"<EOS>\"])\n",
        "\n",
        "        # Truncate if necessary\n",
        "        if len(tokens) > self.max_length:\n",
        "            tokens = tokens[:self.max_length]\n",
        "\n",
        "        # Convert to tensor\n",
        "        caption_tensor = torch.tensor(tokens, dtype=torch.long)\n",
        "\n",
        "        return image, caption_tensor\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Custom collate function to pad captions to same length in batch\"\"\"\n",
        "    images, captions = zip(*batch)\n",
        "    images = torch.stack(images, 0)\n",
        "\n",
        "    # Pad captions\n",
        "    lengths = [len(cap) for cap in captions]\n",
        "    max_len = max(lengths)\n",
        "    padded_captions = torch.zeros(len(captions), max_len, dtype=torch.long)\n",
        "\n",
        "    for i, cap in enumerate(captions):\n",
        "        end = lengths[i]\n",
        "        padded_captions[i, :end] = cap\n",
        "\n",
        "    return images, padded_captions, torch.tensor(lengths)\n",
        "\n",
        "\n",
        "# ==================== MODEL ====================\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "\n",
        "class ResNetEncoder(nn.Module):\n",
        "    def __init__(self, encoded_image_size=14):\n",
        "        super().__init__()\n",
        "        # Load pretrained ResNet-50\n",
        "        resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
        "\n",
        "        # Remove linear and pool layers (keep conv layers)\n",
        "        modules = list(resnet.children())[:-2]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "\n",
        "        # Adaptive pooling to fixed size\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n",
        "\n",
        "    def forward(self, images):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            images: (batch_size, 3, 224, 224)\n",
        "        Returns:\n",
        "            features: (batch_size, 2048, enc_image_size, enc_image_size)\n",
        "        \"\"\"\n",
        "        features = self.resnet(images)  # (batch_size, 2048, H, W)\n",
        "        features = self.adaptive_pool(features)  # (batch_size, 2048, 14, 14)\n",
        "        return features\n",
        "\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=512, nhead=8, num_layers=6,\n",
        "                 dim_feedforward=2048, dropout=0.1, max_length=20):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # Embedding layers\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_length)\n",
        "\n",
        "        # Project image features to d_model\n",
        "        self.image_projection = nn.Linear(2048, d_model)\n",
        "\n",
        "        # Transformer decoder layers\n",
        "        decoder_layer = nn.TransformerDecoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
        "\n",
        "        # Output projection\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, image_features, captions, caption_lengths=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_features: (batch_size, 2048, H, W)\n",
        "            captions: (batch_size, max_caption_len)\n",
        "            caption_lengths: (batch_size,) - actual lengths\n",
        "        Returns:\n",
        "            outputs: (batch_size, caption_len, vocab_size)\n",
        "        \"\"\"\n",
        "        batch_size = image_features.size(0)\n",
        "\n",
        "        # Flatten image features: (batch_size, H*W, 2048)\n",
        "        H, W = image_features.size(2), image_features.size(3)\n",
        "        image_features = image_features.view(batch_size, 2048, H * W)\n",
        "        image_features = image_features.permute(0, 2, 1)  # (batch_size, H*W, 2048)\n",
        "\n",
        "        # Project to d_model\n",
        "        memory = self.image_projection(image_features)  # (batch_size, H*W, d_model)\n",
        "\n",
        "        # Embed captions\n",
        "        caption_len = captions.size(1)\n",
        "        embeddings = self.embedding(captions) * math.sqrt(self.d_model)\n",
        "        embeddings = self.pos_encoder(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "\n",
        "        # Create causal mask (prevents attending to future tokens)\n",
        "        tgt_mask = self.generate_square_subsequent_mask(caption_len).to(captions.device)\n",
        "\n",
        "        # Transformer decoder\n",
        "        decoder_output = self.transformer_decoder(\n",
        "            tgt=embeddings,\n",
        "            memory=memory,\n",
        "            tgt_mask=tgt_mask\n",
        "        )  # (batch_size, caption_len, d_model)\n",
        "\n",
        "        # Project to vocabulary\n",
        "        outputs = self.fc_out(decoder_output)  # (batch_size, caption_len, vocab_size)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        \"\"\"Generate causal mask\"\"\"\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "\n",
        "class ImageCaptioningModel(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=512, nhead=8, num_layers=6):\n",
        "        super().__init__()\n",
        "        self.encoder = ResNetEncoder()\n",
        "        self.decoder = TransformerDecoder(\n",
        "            vocab_size=vocab_size,\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "\n",
        "    def forward(self, images, captions, caption_lengths=None):\n",
        "        image_features = self.encoder(images)\n",
        "        outputs = self.decoder(image_features, captions, caption_lengths)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiLMtaQZx5Nz",
        "outputId": "22aaf49d-4021-40c0-fd9e-8c0f4071bc59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting bert-score\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.9.0+cu126)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.57.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert-score) (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.11.12)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.7.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (3.2.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.3)\n",
            "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate, bert-score\n",
            "Successfully installed bert-score-0.3.13 evaluate-0.4.6\n"
          ]
        }
      ],
      "source": [
        "!pip install evaluate bert-score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate"
      ],
      "metadata": {
        "id": "MiIQeyEjJeqk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Y2Mgy4FyQEC",
        "outputId": "6c49f902-dee1-4c04-cf80-89452735d96b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-pm259xmd\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-pm259xmd\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from clip==1.0)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (25.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (0.24.0+cu126)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->clip==1.0) (0.2.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->clip==1.0) (3.0.3)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=36fdd6aba34d8510545a22218b4cfdc277c8aac7badedfb69d0c758d54f4095e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-v_aln4u6/wheels/35/3e/df/3d24cbfb3b6a06f17a2bfd7d1138900d4365d9028aa8f6e92f\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import clip"
      ],
      "metadata": {
        "id": "aBLUlwrpJ1Rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afABwA3XzNqd",
        "outputId": "a7eb9d7f-ff7d-4ac2-ecd8-7ac42cf6d9e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pycocoevalcap\n",
            "  Downloading pycocoevalcap-1.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from pycocoevalcap) (2.0.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pycocotools>=2.0.2->pycocoevalcap) (2.0.2)\n",
            "Downloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pycocoevalcap\n",
            "Successfully installed pycocoevalcap-1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pycocoevalcap"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from pycocoevalcap.cider.cider import Cider\n",
        "    from pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è  pycocoevalcap not found. Please install: pip install pycocoevalcap\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "cVKpkLEFJtNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7klsNnhhQipZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d53afe55-f307-4fcb-97e3-b2d3b967251f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Evaluation metrics module loaded (BLEU, CIDEr, BERTScore, CLIPScore)\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Evaluation Metrics for Image Captioning\n",
        "Implements BLEU, CIDEr, BERTScore, and CLIPScore metrics\n",
        "\"\"\"\n",
        "class CaptionEvaluator:\n",
        "    def __init__(self, vocab, device=None):\n",
        "        self.vocab = vocab\n",
        "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # Initialize BLEU evaluator (same as BLIP2)\n",
        "        self.bleu_evaluator = evaluate.load(\"bleu\")\n",
        "\n",
        "        # Initialize CIDEr evaluator (same as BLIP2)\n",
        "        self.cider_evaluator = Cider()\n",
        "        self.cider_tokenizer = PTBTokenizer()\n",
        "\n",
        "        # Initialize BERTScore evaluator (same as BLIP2)\n",
        "        self.bertscore_evaluator = evaluate.load(\"bertscore\")\n",
        "\n",
        "        # Initialize CLIP model for CLIPScore (same as BLIP2)\n",
        "        self.clip_model, self.clip_preprocess = clip.load(\"ViT-B/32\", device=self.device)\n",
        "        self.clip_model.eval()\n",
        "\n",
        "    def calculate_bleu(self, references_list, candidates_list):\n",
        "        \"\"\"Calculate BLEU score using evaluate library (same as BLIP2)\"\"\"\n",
        "        bleu_score = self.bleu_evaluator.compute(\n",
        "            predictions=candidates_list,\n",
        "            references=references_list\n",
        "        )\n",
        "        return bleu_score['bleu']  # Single BLEU score (not BLEU-1/2/3/4)\n",
        "\n",
        "    def calculate_cider(self, references_list, candidates_list):\n",
        "        \"\"\"Calculate CIDEr score using official pycocoevalcap (same as BLIP2)\"\"\"\n",
        "        # Prepare in COCO format\n",
        "        gt_dict = {}\n",
        "        res_dict = {}\n",
        "\n",
        "        for i, (pred, refs) in enumerate(zip(candidates_list, references_list)):\n",
        "            gt_dict[str(i)] = [{\"caption\": ref} for ref in refs]\n",
        "            res_dict[str(i)] = [{\"caption\": pred}]\n",
        "\n",
        "        gt_tok = self.cider_tokenizer.tokenize(gt_dict)\n",
        "        res_tok = self.cider_tokenizer.tokenize(res_dict)\n",
        "\n",
        "        cider_score, _ = self.cider_evaluator.compute_score(gt_tok, res_tok)\n",
        "        return float(cider_score)\n",
        "\n",
        "    def calculate_bertscore(self, references_list, candidates_list):\n",
        "        \"\"\"Calculate BERTScore (F1) using evaluate library (same as BLIP2)\"\"\"\n",
        "        # BERTScore expects one reference per prediction\n",
        "        flat_references = [refs[0] for refs in references_list]\n",
        "\n",
        "        bert_results = self.bertscore_evaluator.compute(\n",
        "            predictions=candidates_list,\n",
        "            references=flat_references,\n",
        "            lang=\"en\",\n",
        "            model_type=\"roberta-large\"  # Same as BLIP2\n",
        "        )\n",
        "\n",
        "        bert_f1 = float(np.mean(bert_results[\"f1\"]))\n",
        "        return bert_f1\n",
        "\n",
        "    def calculate_clipscore(self, candidates_list, image_paths):\n",
        "        \"\"\"Calculate CLIPScore using CLIP model (same as BLIP2)\"\"\"\n",
        "        clip_scores = []\n",
        "\n",
        "        for img_path, caption in zip(image_paths, candidates_list):\n",
        "            image = self.clip_preprocess(Image.open(img_path).convert(\"RGB\")).unsqueeze(0).to(self.device)\n",
        "            text = clip.tokenize([caption]).to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                image_features = self.clip_model.encode_image(image)\n",
        "                text_features = self.clip_model.encode_text(text)\n",
        "\n",
        "                # Normalize embeddings\n",
        "                image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "                text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "                # Cosine similarity\n",
        "                score = (image_features * text_features).sum().item()\n",
        "                clip_scores.append(score)\n",
        "\n",
        "        return float(np.mean(clip_scores))\n",
        "\n",
        "    def evaluate_batch(self, references_list, candidates_list, image_paths=None):\n",
        "        \"\"\"\n",
        "        Evaluate a batch of captions with all 4 metrics\n",
        "\n",
        "        Args:\n",
        "            references_list: List of lists, each containing reference captions\n",
        "            candidates_list: List of generated captions\n",
        "            image_paths: List of image paths (required for CLIPScore)\n",
        "\n",
        "        Returns:\n",
        "            avg_results: Dictionary with average scores\n",
        "            per_sample_results: Dictionary with per-sample scores\n",
        "        \"\"\"\n",
        "        # BLEU\n",
        "        bleu_score = self.calculate_bleu(references_list, candidates_list)\n",
        "\n",
        "        # CIDEr\n",
        "        cider_score = self.calculate_cider(references_list, candidates_list)\n",
        "\n",
        "        # BERTScore\n",
        "        bertscore_f1 = self.calculate_bertscore(references_list, candidates_list)\n",
        "\n",
        "        # CLIPScore (if image paths provided)\n",
        "        clipscore = None\n",
        "        if image_paths:\n",
        "            clipscore = self.calculate_clipscore(candidates_list, image_paths)\n",
        "\n",
        "        # Prepare results\n",
        "        avg_results = {\n",
        "            'BLEU': bleu_score,\n",
        "            'CIDEr': cider_score,\n",
        "            'BERTScore': bertscore_f1,\n",
        "            'CLIPScore': clipscore if clipscore is not None else 0.0\n",
        "        }\n",
        "\n",
        "        # Per-sample results (for compatibility)\n",
        "        per_sample_results = {\n",
        "            'BLEU': [bleu_score] * len(candidates_list),\n",
        "            'CIDEr': [cider_score] * len(candidates_list),\n",
        "            'BERTScore': [bertscore_f1] * len(candidates_list),\n",
        "            'CLIPScore': [clipscore] * len(candidates_list) if clipscore is not None else [0.0] * len(candidates_list)\n",
        "        }\n",
        "\n",
        "        return avg_results, per_sample_results\n",
        "\n",
        "print(\"‚úì Evaluation metrics module loaded (BLEU, CIDEr, BERTScore, CLIPScore)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZImmj09FQipZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a447a1e1-be5b-40d8-a14e-34b21d2bd720"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Greedy decoding function loaded\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Greedy Decoding for Caption Generation\n",
        "\"\"\"\n",
        "def generate_caption_greedy(model, image, vocab, device, max_length=20):\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image = image.unsqueeze(0).to(device)\n",
        "        image_features = model.encoder(image)\n",
        "\n",
        "        caption = [vocab.word2idx[\"<SOS>\"]]\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            caption_tensor = torch.LongTensor(caption).unsqueeze(0).to(device)\n",
        "\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                outputs = model.decoder(image_features, caption_tensor)\n",
        "\n",
        "            prediction = outputs[0, -1, :].argmax().item()\n",
        "            caption.append(prediction)\n",
        "\n",
        "            if prediction == vocab.word2idx[\"<EOS>\"]:\n",
        "                break\n",
        "\n",
        "        return vocab.decode(caption)\n",
        "\n",
        "print(\"‚úì Greedy decoding function loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "swmJvWYTQipZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c78ef77-9818-4d2a-f120-0e1c1e625983"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Enhanced trainer with metrics tracking loaded\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Enhanced Trainer with Metrics Tracking and Monitoring\n",
        "\"\"\"\n",
        "\n",
        "class EnhancedTrainer:\n",
        "    def __init__(self, model, train_loader, val_loader, vocab, device,\n",
        "                 lr=2e-4, weight_decay=0.01, label_smoothing=0.1, evaluator=None,\n",
        "                 grad_accum=3):\n",
        "        self.model = model.to(device)\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.vocab = vocab\n",
        "        self.device = device\n",
        "        self.evaluator = evaluator\n",
        "        self.grad_accum = grad_accum  # Gradient accumulation steps (2-4 as per proposal)\n",
        "\n",
        "        # Optimizer\n",
        "        self.optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "        # Loss with label smoothing\n",
        "        self.criterion = nn.CrossEntropyLoss(\n",
        "            ignore_index=vocab.word2idx[\"<PAD>\"],\n",
        "            label_smoothing=label_smoothing\n",
        "        )\n",
        "\n",
        "        # Mixed precision training\n",
        "        self.scaler = torch.amp.GradScaler('cuda')\n",
        "\n",
        "        # Tracking - Use validation loss for early stopping (no heavy metrics during training)\n",
        "        self.best_val_loss = float('inf')  # Loss should be minimized\n",
        "        self.patience_counter = 0\n",
        "\n",
        "        # Metrics history\n",
        "        self.history = {\n",
        "            'train_loss': [],\n",
        "            'val_loss': [],\n",
        "            'train_time': [],\n",
        "            'memory_usage': [],\n",
        "            'epoch_metrics': []  # Will store evaluation metrics per epoch\n",
        "        }\n",
        "\n",
        "    def freeze_encoder(self):\n",
        "        \"\"\"Freeze all encoder parameters\"\"\"\n",
        "        for param in self.model.encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def unfreeze_layer4(self):\n",
        "        \"\"\"Unfreeze layer4 (conv5_x) for fine-tuning\"\"\"\n",
        "        # Access the actual ResNet model structure\n",
        "        # ResNet-50 structure: conv layers -> layer1 -> layer2 -> layer3 -> layer4\n",
        "        # We need to access the last block which is layer4\n",
        "        for name, module in self.model.encoder.resnet.named_children():\n",
        "            if name == 'layer4' or name.startswith('layer4'):\n",
        "                for param in module.parameters():\n",
        "                    param.requires_grad = True\n",
        "        print(\"‚úì Unfroze layer4 for fine-tuning\")\n",
        "\n",
        "        # Also try to unfreeze the last block if layer4 not found\n",
        "        try:\n",
        "            for param in list(self.model.encoder.resnet.children())[-1].parameters():\n",
        "                param.requires_grad = True\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    def get_memory_usage(self):\n",
        "        \"\"\"Get current GPU memory usage in GB\"\"\"\n",
        "        if torch.cuda.is_available():\n",
        "            return torch.cuda.memory_allocated(self.device) / 1e9\n",
        "        return 0.0\n",
        "\n",
        "    def train_epoch(self, epoch):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        # Gradient accumulation: only zero gradients at start of epoch\n",
        "        self.optimizer.zero_grad()\n",
        "        accumulation_step = 0\n",
        "\n",
        "        progress_bar = tqdm(self.train_loader, desc=f\"Epoch {epoch}\")\n",
        "        for batch_idx, (images, captions, lengths) in enumerate(progress_bar):\n",
        "            images = images.to(self.device)\n",
        "            captions = captions.to(self.device)\n",
        "\n",
        "            # Teacher forcing: input is caption[:-1], target is caption[1:]\n",
        "            input_captions = captions[:, :-1]\n",
        "            target_captions = captions[:, 1:]\n",
        "\n",
        "            # Mixed precision forward pass\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                outputs = self.model(images, input_captions)\n",
        "\n",
        "                # Reshape for loss calculation\n",
        "                batch_size, seq_len, vocab_size = outputs.shape\n",
        "                outputs = outputs.reshape(-1, vocab_size)\n",
        "                targets = target_captions.reshape(-1)\n",
        "\n",
        "                loss = self.criterion(outputs, targets)\n",
        "                # Scale loss by accumulation steps\n",
        "                loss = loss / self.grad_accum\n",
        "\n",
        "            # Backward pass with gradient scaling\n",
        "            self.scaler.scale(loss).backward()\n",
        "\n",
        "            accumulation_step += 1\n",
        "\n",
        "            # Update weights every grad_accum steps or at end of epoch\n",
        "            if (accumulation_step % self.grad_accum == 0) or (batch_idx == len(self.train_loader) - 1):\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "            total_loss += loss.item() * self.grad_accum  # Unscale for logging\n",
        "            progress_bar.set_postfix({'loss': loss.item() * self.grad_accum})\n",
        "\n",
        "        avg_loss = total_loss / len(self.train_loader)\n",
        "        epoch_time = time.time() - epoch_start_time\n",
        "        memory_usage = self.get_memory_usage()\n",
        "\n",
        "        return avg_loss, epoch_time, memory_usage\n",
        "\n",
        "    def validate(self):\n",
        "        \"\"\"Validate and compute validation loss only (no heavy metrics during training)\"\"\"\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, captions, lengths in tqdm(self.val_loader, desc=\"Validating\"):\n",
        "                images = images.to(self.device)\n",
        "                captions = captions.to(self.device)\n",
        "\n",
        "                input_captions = captions[:, :-1]\n",
        "                target_captions = captions[:, 1:]\n",
        "\n",
        "                with torch.amp.autocast('cuda'):\n",
        "                    outputs = self.model(images, input_captions)\n",
        "\n",
        "                    batch_size, seq_len, vocab_size = outputs.shape\n",
        "                    outputs_flat = outputs.reshape(-1, vocab_size)\n",
        "                    targets_flat = target_captions.reshape(-1)\n",
        "\n",
        "                    loss = self.criterion(outputs_flat, targets_flat)\n",
        "                    total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(self.val_loader)\n",
        "        return avg_loss\n",
        "\n",
        "    def train(self, num_epochs, early_stopping_patience=5):\n",
        "        print(\"Starting training...\")\n",
        "        print(f\"Epochs 1-5: Encoder frozen, training decoder only\")\n",
        "        print(f\"Epochs 6+: Unfreezing layer4 for fine-tuning\")\n",
        "        print(f\"Gradient accumulation: {self.grad_accum} steps\")\n",
        "        print(f\"Early stopping: Validation loss no improvement for {early_stopping_patience} epochs\")\n",
        "        print(\"Note: Heavy metrics (BLEU, CIDEr, BERTScore, CLIPScore) computed AFTER training only\")\n",
        "\n",
        "        # Initially freeze encoder\n",
        "        self.freeze_encoder()\n",
        "\n",
        "        for epoch in range(1, num_epochs + 1):\n",
        "            # Unfreeze layer4 after epoch 5\n",
        "            if epoch == 6:\n",
        "                self.unfreeze_layer4()\n",
        "\n",
        "            # Train\n",
        "            train_loss, epoch_time, memory_usage = self.train_epoch(epoch)\n",
        "\n",
        "            # Validate - only compute loss (no heavy metrics)\n",
        "            val_loss = self.validate()\n",
        "\n",
        "            # Record history\n",
        "            self.history['train_loss'].append(train_loss)\n",
        "            self.history['val_loss'].append(val_loss)\n",
        "            self.history['train_time'].append(epoch_time)\n",
        "            self.history['memory_usage'].append(memory_usage)\n",
        "\n",
        "            # Print results\n",
        "            print(f\"\\nEpoch {epoch}/{num_epochs}\")\n",
        "            print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "            print(f\"Time: {epoch_time:.2f}s | Memory: {memory_usage:.2f} GB\")\n",
        "\n",
        "            # Early stopping check based on validation loss\n",
        "            if val_loss < self.best_val_loss:\n",
        "                self.best_val_loss = val_loss\n",
        "                self.patience_counter = 0\n",
        "                torch.save(self.model.state_dict(), 'best_model_b.pth')\n",
        "                print(f\"‚úì Saved best model (Val Loss: {val_loss:.4f})\")\n",
        "            else:\n",
        "                self.patience_counter += 1\n",
        "                if self.patience_counter >= early_stopping_patience:\n",
        "                    print(f\"\\nEarly stopping triggered after {epoch} epochs (Val loss no improvement)\")\n",
        "                    break\n",
        "\n",
        "        print(\"\\nTraining complete!\")\n",
        "        return self.history\n",
        "\n",
        "    def plot_training_curves(self):\n",
        "        \"\"\"Plot training and validation loss curves\"\"\"\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "        epochs = range(1, len(self.history['train_loss']) + 1)\n",
        "\n",
        "        # Loss curves\n",
        "        axes[0].plot(epochs, self.history['train_loss'], 'b-', label='Train Loss')\n",
        "        axes[0].plot(epochs, self.history['val_loss'], 'r-', label='Val Loss')\n",
        "        axes[0].set_xlabel('Epoch')\n",
        "        axes[0].set_ylabel('Loss')\n",
        "        axes[0].set_title('Training and Validation Loss')\n",
        "        axes[0].legend()\n",
        "        axes[0].grid(True)\n",
        "\n",
        "        # Memory usage\n",
        "        axes[1].plot(epochs, self.history['memory_usage'], 'g-', label='Memory Usage')\n",
        "        axes[1].set_xlabel('Epoch')\n",
        "        axes[1].set_ylabel('Memory (GB)')\n",
        "        axes[1].set_title('GPU Memory Usage')\n",
        "        axes[1].legend()\n",
        "        axes[1].grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Plot metrics if available\n",
        "        if self.history['epoch_metrics']:\n",
        "            fig, ax = plt.subplots(figsize=(10, 6))\n",
        "            metric_epochs = [m['epoch'] for m in self.history['epoch_metrics']]\n",
        "\n",
        "            metrics_to_plot = ['BLEU', 'CIDEr', 'BERTScore', 'CLIPScore']\n",
        "            for metric in metrics_to_plot:\n",
        "                values = [m[metric] for m in self.history['epoch_metrics'] if metric in m]\n",
        "                if values:\n",
        "                    ax.plot(metric_epochs[:len(values)], values, 'o-', label=metric)\n",
        "\n",
        "            ax.set_xlabel('Epoch')\n",
        "            ax.set_ylabel('Score')\n",
        "            ax.set_title('Evaluation Metrics Over Training')\n",
        "            ax.legend()\n",
        "            ax.grid(True)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "print(\"‚úì Enhanced trainer with metrics tracking loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_ErLbSABQipa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7614f89c-ac21-4492-c9f0-efb4d980515d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Test evaluation functions loaded (BLIP2-style metrics)\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Comprehensive Test Set Evaluation\n",
        "\"\"\"\n",
        "def evaluate_test_set(model, test_loader, vocab, device, evaluator, image_dir='Images'):\n",
        "    \"\"\"Evaluate model on test set with all 4 metrics (BLEU, CIDEr, BERTScore, CLIPScore) using greedy decoding\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_references = []\n",
        "    all_candidates = []\n",
        "    all_image_paths = []\n",
        "\n",
        "    print(\"Evaluating on test set using greedy decoding...\")\n",
        "\n",
        "    dataset = test_loader.dataset  # Flickr8kDataset with .image_dir and .images\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, captions, lengths) in enumerate(tqdm(test_loader)):\n",
        "            images = images.to(device)\n",
        "            captions = captions.to(device)\n",
        "\n",
        "            batch_references = []\n",
        "            batch_image_paths = []\n",
        "\n",
        "            batch_size = images.size(0)\n",
        "            start_idx = batch_idx * test_loader.batch_size  # assumes shuffle=False\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                # Reference caption\n",
        "                ref_caption = vocab.decode(captions[i].cpu().tolist())\n",
        "                batch_references.append([ref_caption])\n",
        "\n",
        "                # Image path for CLIPScore (match BLIP2: image-level evaluation)\n",
        "                img_name = dataset.images[start_idx + i]\n",
        "                img_path = os.path.join(dataset.image_dir, img_name)\n",
        "                batch_image_paths.append(img_path)\n",
        "\n",
        "            # Generate captions using greedy decoding\n",
        "            batch_candidates = []\n",
        "            for i in range(batch_size):\n",
        "                caption = generate_caption_greedy(model, images[i], vocab, device)\n",
        "                batch_candidates.append(caption)\n",
        "\n",
        "            all_references.extend(batch_references)\n",
        "            all_candidates.extend(batch_candidates)\n",
        "            all_image_paths.extend(batch_image_paths)\n",
        "\n",
        "    # Compute metrics (with image paths for CLIPScore)\n",
        "    avg_metrics, per_sample_metrics = evaluator.evaluate_batch(\n",
        "        all_references,\n",
        "        all_candidates,\n",
        "        image_paths=all_image_paths\n",
        "    )\n",
        "\n",
        "    return avg_metrics, per_sample_metrics, all_references, all_candidates, all_image_paths\n",
        "\n",
        "def print_metrics_summary(metrics):\n",
        "    \"\"\"Print formatted metrics summary for 4 metrics\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TEST SET EVALUATION RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\nMetrics:\")\n",
        "    for key in ['BLEU', 'CIDEr', 'BERTScore', 'CLIPScore']:\n",
        "        if key in metrics:\n",
        "            print(f\"  {key}: {metrics[key]:.4f}\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "def evaluate_val_set_first30(model, val_dataset, vocab, device, evaluator, num_images=30):\n",
        "    \"\"\"Evaluate model on first 30 validation images with all 4 metrics and output detailed results\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_references = []\n",
        "    all_candidates = []\n",
        "    all_image_paths = []\n",
        "    all_image_names = []\n",
        "\n",
        "    print(f\"Evaluating on first {num_images} validation images using greedy decoding...\")\n",
        "\n",
        "    # Handle Subset vs regular dataset\n",
        "    if isinstance(val_dataset, Subset):\n",
        "        # Access underlying dataset\n",
        "        underlying_dataset = val_dataset.dataset\n",
        "        dataset_indices = val_dataset.indices\n",
        "        num_images = min(num_images, len(val_dataset))\n",
        "    else:\n",
        "        underlying_dataset = val_dataset\n",
        "        dataset_indices = list(range(len(val_dataset)))\n",
        "        num_images = min(num_images, len(val_dataset))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(num_images)):\n",
        "            # Get image and caption from dataset\n",
        "            image, caption_tensor = val_dataset[i]\n",
        "            image = image.unsqueeze(0).to(device)\n",
        "\n",
        "            # Get reference caption\n",
        "            ref_caption = vocab.decode(caption_tensor.tolist())\n",
        "            all_references.append([ref_caption])\n",
        "\n",
        "            # Get image name and path from underlying dataset\n",
        "            original_idx = dataset_indices[i]\n",
        "            img_name = underlying_dataset.images[original_idx]\n",
        "            all_image_names.append(img_name)\n",
        "            img_path = os.path.join(underlying_dataset.image_dir, img_name)\n",
        "            all_image_paths.append(img_path)\n",
        "\n",
        "            # Generate caption using greedy decoding\n",
        "            gen_caption = generate_caption_greedy(model, image.squeeze(0), vocab, device)\n",
        "            all_candidates.append(gen_caption)\n",
        "\n",
        "    # Compute metrics (with image paths for CLIPScore)\n",
        "    avg_metrics, per_sample_metrics = evaluator.evaluate_batch(\n",
        "        all_references,\n",
        "        all_candidates,\n",
        "        image_paths=all_image_paths\n",
        "    )\n",
        "\n",
        "    return avg_metrics, per_sample_metrics, all_references, all_candidates, all_image_paths, all_image_names\n",
        "\n",
        "def print_val_results_detailed(metrics, per_sample, refs, candidates, img_names, num_images=30):\n",
        "    \"\"\"Print detailed results for validation images\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"VALIDATION SET EVALUATION RESULTS (First {num_images} Images)\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\nAverage Metrics:\")\n",
        "    for key in ['BLEU', 'CIDEr', 'BERTScore', 'CLIPScore']:\n",
        "        if key in metrics:\n",
        "            print(f\"  {key}: {metrics[key]:.4f}\")\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*80)\n",
        "    print(\"DETAILED RESULTS FOR EACH IMAGE:\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    for i in range(min(num_images, len(img_names))):\n",
        "        print(f\"\\nImage {i+1}: {img_names[i]}\")\n",
        "        print(f\"  Reference: {refs[i][0]}\")\n",
        "        print(f\"  Generated: {candidates[i]}\")\n",
        "        if per_sample and 'BLEU' in per_sample:\n",
        "            # Note: per_sample metrics are averaged, so we'll show the average\n",
        "            print(f\"  Metrics: BLEU={metrics.get('BLEU', 0):.4f}, CIDEr={metrics.get('CIDEr', 0):.4f}, \"\n",
        "                  f\"BERTScore={metrics.get('BERTScore', 0):.4f}, CLIPScore={metrics.get('CLIPScore', 0):.4f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "print(\"‚úì Test evaluation functions loaded (BLIP2-style metrics)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "IEKEbDegQipa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c93d3cbc-c701-4e32-c837-88426e7b6576"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Visualization functions loaded\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Visualization Functions for Image Captioning\n",
        "\"\"\"\n",
        "\n",
        "def visualize_captions(model, test_dataset, vocab, device, num_samples=8):\n",
        "    \"\"\"Visualize generated captions on sample images using greedy decoding\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Sample random indices\n",
        "    indices = np.random.choice(len(test_dataset), num_samples, replace=False)\n",
        "\n",
        "    fig = plt.figure(figsize=(16, 2 * num_samples))\n",
        "    gs = gridspec.GridSpec(num_samples, 1, hspace=0.3)\n",
        "\n",
        "    for idx, sample_idx in enumerate(indices):\n",
        "        image, caption_tensor = test_dataset[sample_idx]\n",
        "\n",
        "        # Get reference caption\n",
        "        ref_caption = vocab.decode(caption_tensor.tolist())\n",
        "\n",
        "        # Generate caption using greedy decoding\n",
        "        gen_caption = generate_caption_greedy(model, image, vocab, device)\n",
        "\n",
        "        # Display\n",
        "        ax = fig.add_subplot(gs[idx])\n",
        "\n",
        "        # Convert image tensor to displayable format\n",
        "        img_display = image.clone()\n",
        "        img_display = img_display.permute(1, 2, 0)\n",
        "        img_display = img_display * torch.tensor([0.229, 0.224, 0.225]) + torch.tensor([0.485, 0.456, 0.406])\n",
        "        img_display = torch.clamp(img_display, 0, 1)\n",
        "\n",
        "        ax.imshow(img_display.cpu().numpy())\n",
        "        ax.axis('off')\n",
        "\n",
        "        # Add captions\n",
        "        ax.text(0.02, 0.98, f\"Reference: {ref_caption}\",\n",
        "                transform=ax.transAxes, fontsize=10, verticalalignment='top',\n",
        "                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))\n",
        "        ax.text(0.02, 0.85, f\"Generated: {gen_caption}\",\n",
        "                transform=ax.transAxes, fontsize=10, verticalalignment='top',\n",
        "                bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\n",
        "\n",
        "    plt.suptitle('Caption Generation Examples (Greedy Decoding)',\n",
        "                 fontsize=14, y=0.995)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_attention_heatmap(model, image, caption, vocab, device, layer_idx=0):\n",
        "    \"\"\"Visualize attention weights from transformer decoder\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # This is a simplified attention visualization\n",
        "    # Full attention visualization would require modifying the model to return attention weights\n",
        "    print(\"Note: Full attention visualization requires model modification to return attention weights\")\n",
        "    print(\"This is a placeholder for attention visualization\")\n",
        "\n",
        "    # For now, just show the image and caption\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
        "\n",
        "    img_display = image.clone()\n",
        "    img_display = img_display.permute(1, 2, 0)\n",
        "    img_display = img_display * torch.tensor([0.229, 0.224, 0.225]) + torch.tensor([0.485, 0.456, 0.406])\n",
        "    img_display = torch.clamp(img_display, 0, 1)\n",
        "\n",
        "    ax.imshow(img_display.cpu().numpy())\n",
        "    ax.axis('off')\n",
        "    ax.set_title(f\"Caption: {caption}\", fontsize=12, wrap=True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def visualize_captions(model, test_dataset, vocab, device, num_samples=5):\n",
        "    \"\"\"Visualize generated captions on sample images\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    indices = np.random.choice(len(test_dataset), num_samples, replace=False)\n",
        "\n",
        "    fig = plt.figure(figsize=(18, 3 * num_samples))\n",
        "\n",
        "    for idx, sample_idx in enumerate(indices):\n",
        "        image, caption_tensor = test_dataset[sample_idx]\n",
        "\n",
        "        ref_caption = vocab.decode(caption_tensor.tolist())\n",
        "\n",
        "        # Generate caption using greedy decoding\n",
        "        generated_caption = generate_caption_greedy(model, image, vocab, device)\n",
        "\n",
        "        # Display\n",
        "        ax = fig.add_subplot(num_samples, 1, idx + 1)\n",
        "\n",
        "        img_display = image.clone()\n",
        "        img_display = img_display.permute(1, 2, 0)\n",
        "        img_display = img_display * torch.tensor([0.229, 0.224, 0.225]) + torch.tensor([0.485, 0.456, 0.406])\n",
        "        img_display = torch.clamp(img_display, 0, 1)\n",
        "\n",
        "        ax.imshow(img_display.cpu().numpy())\n",
        "        ax.axis('off')\n",
        "\n",
        "        text = f\"Reference: {ref_caption}\\n\"\n",
        "        text += f\"Generated: {generated_caption}\"\n",
        "\n",
        "        ax.text(0.02, 0.98, text, transform=ax.transAxes, fontsize=9,\n",
        "                verticalalignment='top',\n",
        "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
        "\n",
        "    plt.suptitle('Generated Captions', fontsize=14, y=0.995)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_error_analysis(metrics_dict, references, candidates, top_k=10):\n",
        "    \"\"\"Plot examples with highest and lowest scores for error analysis\"\"\"\n",
        "    # Sort by BLEU score\n",
        "    bleu_scores = metrics_dict.get('BLEU', [])\n",
        "    if not bleu_scores:\n",
        "        print(\"No BLEU scores available for error analysis\")\n",
        "        return\n",
        "\n",
        "    sorted_indices = np.argsort(bleu_scores)\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"ERROR ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\nTop {top_k} Best Predictions:\")\n",
        "    for i in sorted_indices[-top_k:][::-1]:\n",
        "        print(f\"\\nSample {i}:\")\n",
        "        print(f\"  Reference: {references[i][0]}\")\n",
        "        print(f\"  Generated: {candidates[i]}\")\n",
        "        print(f\"  BLEU: {bleu_scores[i]:.4f}\")\n",
        "\n",
        "    print(f\"\\n\\nTop {top_k} Worst Predictions:\")\n",
        "    for i in sorted_indices[:top_k]:\n",
        "        print(f\"\\nSample {i}:\")\n",
        "        print(f\"  Reference: {references[i][0]}\")\n",
        "        print(f\"  Generated: {candidates[i]}\")\n",
        "        print(f\"  BLEU: {bleu_scores[i]:.4f}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "print(\"‚úì Visualization functions loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "XuY1zbLpQipa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "477375e9-eafa-4d8f-d5db-e0dbe2ae285d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Ablation study functions loaded\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Ablation Studies: Compare Different Model Configurations\n",
        "\"\"\"\n",
        "def run_ablation_study(base_config, variations, train_loader, val_loader, vocab, device,\n",
        "                       num_epochs=10, evaluator=None):\n",
        "    \"\"\"\n",
        "    Run ablation study comparing different model configurations\n",
        "\n",
        "    Args:\n",
        "        base_config: Base hyperparameters dict\n",
        "        variations: List of dicts with variations to test\n",
        "        train_loader, val_loader: Data loaders\n",
        "        vocab: Vocabulary\n",
        "        device: Device\n",
        "        num_epochs: Number of epochs per configuration\n",
        "        evaluator: CaptionEvaluator instance\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for i, variation in enumerate(variations):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Ablation Study {i+1}/{len(variations)}\")\n",
        "        print(f\"Configuration: {variation}\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "\n",
        "        # Merge base config with variation\n",
        "        config = {**base_config, **variation}\n",
        "\n",
        "        # Create model with this configuration\n",
        "        model = ImageCaptioningModel(\n",
        "            vocab_size=len(vocab),\n",
        "            d_model=config.get('d_model', 512),\n",
        "            nhead=config.get('nhead', 8),\n",
        "            num_layers=config.get('num_layers', 6)\n",
        "        )\n",
        "\n",
        "        # Count parameters\n",
        "        total_params = sum(p.numel() for p in model.parameters())\n",
        "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "        # Train\n",
        "        trainer = EnhancedTrainer(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            vocab=vocab,\n",
        "            device=device,\n",
        "            lr=config.get('lr', 2e-4),\n",
        "            weight_decay=config.get('weight_decay', 0.01),\n",
        "            evaluator=evaluator,\n",
        "            grad_accum=config.get('grad_accum', 3)  # Default gradient accumulation\n",
        "        )\n",
        "\n",
        "        history = trainer.train(num_epochs=num_epochs)\n",
        "\n",
        "        # Get final metrics\n",
        "        final_val_loss = history['val_loss'][-1]\n",
        "        final_metrics = history['epoch_metrics'][-1] if history['epoch_metrics'] else {}\n",
        "\n",
        "        results.append({\n",
        "            'config': variation,\n",
        "            'total_params': total_params,\n",
        "            'trainable_params': trainable_params,\n",
        "            'final_val_loss': final_val_loss,\n",
        "            'final_metrics': final_metrics,\n",
        "            'history': history\n",
        "        })\n",
        "\n",
        "        # Clean up\n",
        "        del model\n",
        "        del trainer\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return results\n",
        "\n",
        "def print_ablation_results(results):\n",
        "    \"\"\"Print ablation study results in a formatted table\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ABLATION STUDY RESULTS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    for i, result in enumerate(results):\n",
        "        print(f\"\\nConfiguration {i+1}: {result['config']}\")\n",
        "        print(f\"  Parameters: {result['total_params']:,} total, {result['trainable_params']:,} trainable\")\n",
        "        print(f\"  Final Val Loss: {result['final_val_loss']:.4f}\")\n",
        "\n",
        "        if result['final_metrics']:\n",
        "            print(\"  Final Metrics:\")\n",
        "            for key, value in result['final_metrics'].items():\n",
        "                if key != 'epoch':\n",
        "                    print(f\"    {key}: {value:.4f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "def plot_ablation_comparison(results, metric='BLEU'):\n",
        "    \"\"\"Plot comparison of different configurations\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    config_names = [str(r['config']) for r in results]\n",
        "    val_losses = [r['final_val_loss'] for r in results]\n",
        "\n",
        "    # Plot validation losses\n",
        "    axes[0].bar(range(len(results)), val_losses)\n",
        "    axes[0].set_xlabel('Configuration')\n",
        "    axes[0].set_ylabel('Validation Loss')\n",
        "    axes[0].set_title('Validation Loss by Configuration')\n",
        "    axes[0].set_xticks(range(len(results)))\n",
        "    axes[0].set_xticklabels([f'Config {i+1}' for i in range(len(results))], rotation=45)\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot metrics if available\n",
        "    if metric and results[0]['final_metrics']:\n",
        "        metric_values = []\n",
        "        for r in results:\n",
        "            if metric in r['final_metrics']:\n",
        "                metric_values.append(r['final_metrics'][metric])\n",
        "            else:\n",
        "                metric_values.append(0.0)\n",
        "\n",
        "        axes[1].bar(range(len(results)), metric_values)\n",
        "        axes[1].set_xlabel('Configuration')\n",
        "        axes[1].set_ylabel(metric)\n",
        "        axes[1].set_title(f'{metric} by Configuration')\n",
        "        axes[1].set_xticks(range(len(results)))\n",
        "        axes[1].set_xticklabels([f'Config {i+1}' for i in range(len(results))], rotation=45)\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"‚úì Ablation study functions loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "JzQkO_lGQipa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6f6cb4274e5d430fb27ba835acc97b71",
            "828299a2de0343279c24ee83c6ee19d4",
            "cce0098408f54e54a640d1b4f2e1c31b",
            "ee40094f6c3344248c52877e1dba89dc",
            "88766b35f21d47bea107022f91a0a750",
            "fde7cf5ef1cc4795a4b88aed117c2449",
            "9ec485a4cc7c423785be229d03d422fa",
            "9bd0d9ad7e444ff4916c2fe3328e1529",
            "f3d345f7a5104992ad44812c7f9fae1d",
            "ed62a1e33a974d1fabd3301ab70ac0bc",
            "54c027fdc50e49749c377bf1b5e6b474",
            "9ed3351658f444a698667be201ed0427",
            "baec5f9d64334664bbfac39eb8ff8ce7",
            "56e18f478e9544db981dedea22c6a325",
            "06501b3e310346d0b8ce440071bc3f48",
            "f6b7242606e3438f83a17c90a7cc83b4",
            "0e9d1c735c014182afa7749d68efb958",
            "ccd3147f6c5b461ebbdcad0cd1d740f7",
            "17e7946330854e4e89e83ed5e6834151",
            "ee05a617f1514c05a8e25d6133eccd11",
            "2d1dcd52b58f4e849988b6b081e1f9a3",
            "1a8d22fbe7d647e0b4e9d109341f43dc",
            "2b1a078981a545eabb71718b8e0b5b44",
            "3d84afc5db384936aa19311b115a0143",
            "0a26e2e959b4490ebe5f91a7f16301c8",
            "45f99c55942a4037bdfc7e9e72540bd2",
            "48ef7e9c2ee44ef0926fe6e34013abf5",
            "f26645541d98445f9b8f2b46169cc902",
            "028b501c80484e8eaa458477e2099f39",
            "6740e4e3701047eba0a70ecc431d0ec2",
            "5ef1f3a355604090a290ab9161529c9e",
            "6208cf531ec94b2b8380d57fb6573fdb",
            "62a1f563f5b949529a6a775558fe9641",
            "8fb705b8c1be4c149dc282766afea7f4",
            "490c3b7c194e4c9987f8d040a96bdf2d",
            "b09ec53898224e72aa7fb2919a26919e",
            "970be341b485499a9fbd931036334b2d",
            "71c1b4dbf31c430bbd321c148ef72786",
            "869047f047a6439f84664969e192c5d1",
            "00a12b5aa6fa4d71a3bb5939bcd71e5a",
            "d806064075e14a90aa95611295017b1e",
            "3e710baaf84844b6a23b0eba44cec1a5",
            "14c7af6dd6e84ace8b78ce74059c7d57",
            "e225c13b57694958a611fa47425db521"
          ]
        },
        "outputId": "4a9cf076-0454-46e6-c476-ec869a46b444"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "GPU: Tesla T4\n",
            "Memory: 15.83 GB\n",
            "\n",
            "============================================================\n",
            "Building vocabulary...\n",
            "============================================================\n",
            "Vocabulary size: 4601\n",
            "\n",
            "============================================================\n",
            "Loading datasets...\n",
            "============================================================\n",
            "Validation dataset limited to first 30 images (out of 5000 total)\n",
            "Train samples: 30000\n",
            "Val samples: 30\n",
            "Test samples: 5000\n",
            "\n",
            "============================================================\n",
            "Initializing Model B (ResNet-50 + Transformer Decoder)...\n",
            "============================================================\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97.8M/97.8M [00:00<00:00, 133MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 54,497,337\n",
            "Trainable parameters: 54,497,337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f6cb4274e5d430fb27ba835acc97b71"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ed3351658f444a698667be201ed0427"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading extra modules: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2b1a078981a545eabb71718b8e0b5b44"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8fb705b8c1be4c149dc282766afea7f4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 338M/338M [00:02<00:00, 125MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Starting Training...\n",
            "============================================================\n",
            "Starting training...\n",
            "Epochs 1-5: Encoder frozen, training decoder only\n",
            "Epochs 6+: Unfreezing layer4 for fine-tuning\n",
            "Gradient accumulation: 3 steps\n",
            "Early stopping: Validation loss no improvement for 5 epochs\n",
            "Note: Heavy metrics (BLEU, CIDEr, BERTScore, CLIPScore) computed AFTER training only\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [03:19<00:00,  4.69it/s, loss=4.24]\n",
            "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/20\n",
            "Train Loss: 4.5010 | Val Loss: 3.8255\n",
            "Time: 199.95s | Memory: 0.87 GB\n",
            "‚úì Saved best model (Val Loss: 3.8255)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [03:11<00:00,  4.91it/s, loss=3.28]\n",
            "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2/20\n",
            "Train Loss: 3.7614 | Val Loss: 3.6595\n",
            "Time: 191.19s | Memory: 0.87 GB\n",
            "‚úì Saved best model (Val Loss: 3.6595)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [03:10<00:00,  4.91it/s, loss=3.45]\n",
            "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3/20\n",
            "Train Loss: 3.5088 | Val Loss: 3.4704\n",
            "Time: 190.90s | Memory: 0.87 GB\n",
            "‚úì Saved best model (Val Loss: 3.4704)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [03:12<00:00,  4.88it/s, loss=3.32]\n",
            "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4/20\n",
            "Train Loss: 3.3285 | Val Loss: 3.4771\n",
            "Time: 192.11s | Memory: 0.87 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [03:12<00:00,  4.88it/s, loss=3.39]\n",
            "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5/20\n",
            "Train Loss: 3.1765 | Val Loss: 3.5005\n",
            "Time: 192.40s | Memory: 0.87 GB\n",
            "‚úì Unfroze layer4 for fine-tuning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [03:15<00:00,  4.79it/s, loss=2.92]\n",
            "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6/20\n",
            "Train Loss: 3.0813 | Val Loss: 3.3766\n",
            "Time: 195.98s | Memory: 1.00 GB\n",
            "‚úì Saved best model (Val Loss: 3.3766)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [03:14<00:00,  4.81it/s, loss=2.89]\n",
            "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7/20\n",
            "Train Loss: 2.8943 | Val Loss: 3.4647\n",
            "Time: 195.00s | Memory: 1.00 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [03:16<00:00,  4.77it/s, loss=2.8]\n",
            "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8/20\n",
            "Train Loss: 2.7516 | Val Loss: 3.4916\n",
            "Time: 196.61s | Memory: 1.00 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [03:16<00:00,  4.77it/s, loss=2.78]\n",
            "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9/20\n",
            "Train Loss: 2.6260 | Val Loss: 3.5112\n",
            "Time: 196.48s | Memory: 1.00 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [03:17<00:00,  4.76it/s, loss=2.78]\n",
            "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10/20\n",
            "Train Loss: 2.5122 | Val Loss: 3.5455\n",
            "Time: 197.15s | Memory: 1.00 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 938/938 [03:15<00:00,  4.79it/s, loss=2.62]\n",
            "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 11/20\n",
            "Train Loss: 2.4067 | Val Loss: 3.6352\n",
            "Time: 195.97s | Memory: 1.00 GB\n",
            "\n",
            "Early stopping triggered after 11 epochs (Val loss no improvement)\n",
            "\n",
            "Training complete!\n",
            "\n",
            "Total training time: 35.84 minutes\n",
            "\n",
            "============================================================\n",
            "Plotting Training Curves...\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Loading best model for evaluation...\n",
            "============================================================\n",
            "‚úì Model B training complete!\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Main Training and Evaluation Pipeline for Model B\n",
        "This cell demonstrates the complete workflow with all features\n",
        "\"\"\"\n",
        "\n",
        "# Hyperparameters\n",
        "BATCH_SIZE = 32  # Within 16-32 range as per proposal\n",
        "GRAD_ACCUM = 3  # Gradient accumulation steps (2-4 as per proposal)\n",
        "LEARNING_RATE = 2e-4\n",
        "WEIGHT_DECAY = 0.01\n",
        "NUM_EPOCHS = 20\n",
        "D_MODEL = 512\n",
        "NHEAD = 8\n",
        "NUM_LAYERS = 6\n",
        "MAX_LENGTH = 20\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "# Image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Initialize vocabulary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Building vocabulary...\")\n",
        "print(\"=\"*60)\n",
        "vocab = Vocabulary(freq_threshold=2)\n",
        "\n",
        "all_captions = []\n",
        "with open('flickr8k_train_captions.txt', 'r') as f:\n",
        "    for line in f:\n",
        "        parts = line.strip().split('\\t')\n",
        "        if len(parts) == 2:\n",
        "            all_captions.append(parts[1])\n",
        "\n",
        "vocab.build_vocabulary(all_captions)\n",
        "print(f\"Vocabulary size: {len(vocab)}\")\n",
        "\n",
        "# Create datasets\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Loading datasets...\")\n",
        "print(\"=\"*60)\n",
        "train_dataset = Flickr8kDataset(\n",
        "    image_dir='Images',\n",
        "    captions_file='flickr8k_train_captions.txt',\n",
        "    vocab=vocab,\n",
        "    transform=transform,\n",
        "    max_length=MAX_LENGTH\n",
        ")\n",
        "\n",
        "# Create full validation dataset\n",
        "val_dataset_full = Flickr8kDataset(\n",
        "    image_dir='Images',\n",
        "    captions_file='flickr8k_val_captions.txt',\n",
        "    vocab=vocab,\n",
        "    transform=transform,\n",
        "    max_length=MAX_LENGTH\n",
        ")\n",
        "\n",
        "# Limit validation dataset to first 30 images only\n",
        "val_dataset = Subset(val_dataset_full, indices=list(range(30)))\n",
        "print(f\"Validation dataset limited to first 30 images (out of {len(val_dataset_full)} total)\")\n",
        "\n",
        "test_dataset = Flickr8kDataset(\n",
        "    image_dir='Images',\n",
        "    captions_file='flickr8k_test_captions.txt',\n",
        "    vocab=vocab,\n",
        "    transform=transform,\n",
        "    max_length=MAX_LENGTH\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=2,  # Reduced to avoid warnings\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Val samples: {len(val_dataset)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")\n",
        "\n",
        "# Initialize model\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Initializing Model B (ResNet-50 + Transformer Decoder)...\")\n",
        "print(\"=\"*60)\n",
        "model = ImageCaptioningModel(\n",
        "    vocab_size=len(vocab),\n",
        "    d_model=D_MODEL,\n",
        "    nhead=NHEAD,\n",
        "    num_layers=NUM_LAYERS\n",
        ")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "# Initialize evaluator\n",
        "evaluator = CaptionEvaluator(vocab, device=device)\n",
        "\n",
        "# Train model\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Starting Training...\")\n",
        "print(\"=\"*60)\n",
        "trainer = EnhancedTrainer(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    vocab=vocab,\n",
        "    device=device,\n",
        "    lr=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    evaluator=evaluator,\n",
        "    grad_accum=GRAD_ACCUM  # Gradient accumulation as per proposal\n",
        ")\n",
        "\n",
        "training_start_time = time.time()\n",
        "history = trainer.train(num_epochs=NUM_EPOCHS, early_stopping_patience=5)\n",
        "total_training_time = time.time() - training_start_time\n",
        "\n",
        "print(f\"\\nTotal training time: {total_training_time / 60:.2f} minutes\")\n",
        "\n",
        "# Plot training curves\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Plotting Training Curves...\")\n",
        "print(\"=\"*60)\n",
        "trainer.plot_training_curves()\n",
        "\n",
        "# Load best model for evaluation\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Loading best model for evaluation...\")\n",
        "print(\"=\"*60)\n",
        "model.load_state_dict(torch.load('best_model_b.pth'))\n",
        "model = model.to(device)\n",
        "\n",
        "print(\"‚úì Model B training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "etXUUpVPQipa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e47da566-b4b2-46e8-a730-6b35a5ae5cf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "TEST SET EVALUATION\n",
            "============================================================\n",
            "\n",
            "Evaluating with Greedy Decoding...\n",
            "Evaluating on test set using greedy decoding...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157/157 [08:59<00:00,  3.43s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "TEST SET EVALUATION RESULTS\n",
            "============================================================\n",
            "\n",
            "Metrics:\n",
            "  BLEU: 0.0715\n",
            "  CIDEr: 0.5407\n",
            "  BERTScore: 0.9052\n",
            "  CLIPScore: 0.2766\n",
            "============================================================\n",
            "\n",
            "\n",
            "‚úì Full inference results saved to model_b_results.json\n",
            "  - 5000 samples\n",
            "  - Generated captions: 5000\n",
            "  - Image paths: 5000\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Comprehensive Test Set Evaluation\n",
        "Evaluate the trained model on test set with all 4 metrics (BLEU, CIDEr, BERTScore, CLIPScore)\n",
        "All metrics computed AFTER training for efficiency\n",
        "\"\"\"\n",
        "print(\"=\"*60)\n",
        "print(\"TEST SET EVALUATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Evaluate with greedy decoding\n",
        "print(\"\\nEvaluating with Greedy Decoding...\")\n",
        "metrics, per_sample, refs, candidates, imgs = evaluate_test_set(\n",
        "    model, test_loader, vocab, device, evaluator\n",
        ")\n",
        "print_metrics_summary(metrics)\n",
        "\n",
        "# Save full inference results\n",
        "results_summary = {\n",
        "    # Average metrics\n",
        "    'metrics': metrics,\n",
        "\n",
        "    # Per-sample metrics\n",
        "    'per_sample_metrics': per_sample,\n",
        "\n",
        "    # Generated captions (full inference results)\n",
        "    'references': refs,  # List of reference captions (one per image)\n",
        "    'generated_captions': candidates,  # List of captions generated with greedy decoding\n",
        "\n",
        "    # Image paths\n",
        "    'image_paths': imgs if imgs else [],  # List of image file paths/identifiers\n",
        "\n",
        "    # Model configuration\n",
        "    'model_config': {\n",
        "        'd_model': D_MODEL,\n",
        "        'nhead': NHEAD,\n",
        "        'num_layers': NUM_LAYERS,\n",
        "        'total_params': total_params\n",
        "    },\n",
        "\n",
        "    # Metadata\n",
        "    'num_samples': len(refs) if refs else 0,\n",
        "    'inference_method': 'greedy'\n",
        "}\n",
        "\n",
        "with open('model_b_results.json', 'w') as f:\n",
        "    json.dump(results_summary, f, indent=2)\n",
        "print(\"\\n‚úì Full inference results saved to model_b_results.json\")\n",
        "print(f\"  - {len(refs) if refs else 0} samples\")\n",
        "print(f\"  - Generated captions: {len(candidates) if candidates else 0}\")\n",
        "print(f\"  - Image paths: {len(imgs) if imgs else 0}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5_I2Wwf5H9S",
        "outputId": "82171500-45cd-45cf-bfe1-725ed096ef99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image_to_captions rebuilt.\n",
            "Example: [('image', ['caption'])]\n"
          ]
        }
      ],
      "source": [
        "# =========================================\n",
        "# Rebuild image_to_captions (GT captions)\n",
        "# =========================================\n",
        "\n",
        "caption_df = pd.read_csv(\n",
        "    \"captions.txt\",\n",
        "    names=[\"image_file\", \"caption\"]\n",
        ")\n",
        "\n",
        "image_to_captions = defaultdict(list)\n",
        "for _, row in caption_df.iterrows():\n",
        "    image_to_captions[row[\"image_file\"]].append(row[\"caption\"])\n",
        "\n",
        "print(\"image_to_captions rebuilt.\")\n",
        "print(\"Example:\", list(image_to_captions.items())[:1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "tNeM0ABp5H9S"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "SAVE_ROOT = \"/content/drive/MyDrive/UPenn 2025 fall courses/ESE 5460/proj/eva_analysis\"\n",
        "IMAGE_DIR = \"Images\"\n",
        "CANVAS_W = 400\n",
        "CANVAS_H = 140\n",
        "LEFT_W = 200\n",
        "RIGHT_W = 200\n",
        "FONT_SIZE = 50\n",
        "LINE_SPACING = 10\n",
        "\n",
        "# ===============================\n",
        "# FONT\n",
        "# ===============================\n",
        "def load_font(size):\n",
        "    try:\n",
        "        return ImageFont.truetype(\n",
        "            \"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\",\n",
        "            size\n",
        "        )\n",
        "    except:\n",
        "        return ImageFont.load_default()\n",
        "\n",
        "font = load_font(FONT_SIZE)\n",
        "\n",
        "# ===============================\n",
        "# TEXT DRAWING\n",
        "# ===============================\n",
        "def draw_wrapped_text(draw, text, x, y, max_chars):\n",
        "    lines = textwrap.wrap(text, max_chars)\n",
        "    cur_y = y\n",
        "    for line in lines:\n",
        "        draw.text((x, cur_y), line, fill=(0, 0, 0), font=font)\n",
        "        cur_y += font.size + LINE_SPACING\n",
        "\n",
        "# ===============================\n",
        "# CREATE PNG (NO WHITESPACE)\n",
        "# ===============================\n",
        "def create_png(image, caption):\n",
        "    canvas = Image.new(\"RGB\", (CANVAS_W, CANVAS_H), (255, 255, 255))\n",
        "    draw = ImageDraw.Draw(canvas)\n",
        "\n",
        "    # ---- Image fills LEFT half ----\n",
        "    image = image.resize((LEFT_W, CANVAS_H))\n",
        "    canvas.paste(image, (0, 0))\n",
        "\n",
        "    # ---- Caption fills RIGHT half ----\n",
        "    draw_wrapped_text(\n",
        "        draw,\n",
        "        caption,\n",
        "        x=LEFT_W + 12,\n",
        "        y=20,\n",
        "        max_chars=30\n",
        "    )\n",
        "    return canvas\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the path to the uploaded .pth file\n",
        "model_path = '/content/best_model_b.pth'\n",
        "\n",
        "# Load the model checkpoint\n",
        "try:\n",
        "    loaded_model_state = torch.load(model_path)\n",
        "    print(\"Model state loaded successfully!\")\n",
        "    # You can inspect the loaded state, for example:\n",
        "    # print(loaded_model_state.keys())\n",
        "except Exception as e:\n",
        "    print(f\"Error loading the model state: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kho05P4Sdim",
        "outputId": "6638b816-5f0d-48fd-c140-4f3460bcbdfd"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model state loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "for i in range(30):\n",
        "    img_name = val_images[i]\n",
        "    img_path = os.path.join(IMAGE_DIR, img_name)\n",
        "\n",
        "    # Load image\n",
        "    image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "    # Transform to tensor (but don't add batch dimension yet - function does that)\n",
        "    image_tensor = transform(image)  # Remove .to(device) - function handles this\n",
        "\n",
        "    # Generate caption\n",
        "    with torch.no_grad():\n",
        "        caption = generate_caption_greedy(model, image_tensor, vocab, device)\n",
        "\n",
        "    # Create visualization\n",
        "    canvas = create_png(image, caption)\n",
        "\n",
        "    # Save\n",
        "    save_path = os.path.join(SAVE_DIR, f\"val_{i:02d}.png\")\n",
        "    canvas.save(save_path)\n",
        "\n",
        "    print(f\"‚úî saved {save_path}\")\n",
        "\n",
        "print(\"‚úÖ Qualitative evaluation images generated.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XO9OiDXuS1Z8",
        "outputId": "c050613b-c572-4152-e732-309dc025950c"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úî saved /content/drive/MyDrive/eva_qualitative/val_00.png\n",
            "‚úî saved /content/drive/MyDrive/eva_qualitative/val_01.png\n",
            "‚úî saved /content/drive/MyDrive/eva_qualitative/val_02.png\n",
            "‚úî saved /content/drive/MyDrive/eva_qualitative/val_03.png\n",
            "‚úî saved /content/drive/MyDrive/eva_qualitative/val_04.png\n",
            "‚úî saved /content/drive/MyDrive/eva_qualitative/val_05.png\n",
            "‚úî saved /content/drive/MyDrive/eva_qualitative/val_06.png\n",
            "‚úî saved /content/drive/MyDrive/eva_qualitative/val_07.png\n",
            "‚úî saved /content/drive/MyDrive/eva_qualitative/val_08.png\n",
            "‚úî saved /content/drive/MyDrive/eva_qualitative/val_09.png\n",
            "‚úî saved /content/drive/MyDrive/eva_qualitative/val_10.png\n",
            "‚úî saved /content/drive/MyDrive/eva_qualitative/val_11.png\n",
            "‚úî saved /content/drive/MyDrive/eva_qualitative/val_12.png\n",
            "‚úî saved /content/drive/MyDrive/eva_qualitative/val_13.png\n",
            "‚úî saved /content/drive/MyDrive/eva_qualitative/val_14.png\n",
            "‚úî saved /content/drive/MyDrive/eva_qualitative/val_15.png\n",
            "‚úî saved /content/drive/MyDrive/eva_qualitative/val_16.png\n",
            "‚úî saved /content/drive/MyDrive/eva_qualitative/val_17.png\n",
            "‚úî saved /content/drive/MyDrive/eva_qualitative/val_18.png\n",
            "‚úî saved /content/drive/MyDrive/eva_qualitative/val_19.png\n",
            "‚úî saved /content/drive/MyDrive/eva_qualitative/val_20.png\n",
            "‚úî saved /content/drive/MyDrive/eva_qualitative/val_21.png\n",
            "‚úî saved /content/drive/MyDrive/eva_qualitative/val_22.png\n",
            "‚úî saved /content/drive/MyDrive/eva_qualitative/val_23.png\n",
            "‚úî saved /content/drive/MyDrive/eva_qualitative/val_24.png\n",
            "‚úî saved /content/drive/MyDrive/eva_qualitative/val_25.png\n",
            "‚úî saved /content/drive/MyDrive/eva_qualitative/val_26.png\n",
            "‚úî saved /content/drive/MyDrive/eva_qualitative/val_27.png\n",
            "‚úî saved /content/drive/MyDrive/eva_qualitative/val_28.png\n",
            "‚úî saved /content/drive/MyDrive/eva_qualitative/val_29.png\n",
            "‚úÖ Qualitative evaluation images generated.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6f6cb4274e5d430fb27ba835acc97b71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_828299a2de0343279c24ee83c6ee19d4",
              "IPY_MODEL_cce0098408f54e54a640d1b4f2e1c31b",
              "IPY_MODEL_ee40094f6c3344248c52877e1dba89dc"
            ],
            "layout": "IPY_MODEL_88766b35f21d47bea107022f91a0a750"
          }
        },
        "828299a2de0343279c24ee83c6ee19d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fde7cf5ef1cc4795a4b88aed117c2449",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9ec485a4cc7c423785be229d03d422fa",
            "value": "Downloading‚Äábuilder‚Äáscript:‚Äá"
          }
        },
        "cce0098408f54e54a640d1b4f2e1c31b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9bd0d9ad7e444ff4916c2fe3328e1529",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f3d345f7a5104992ad44812c7f9fae1d",
            "value": 1
          }
        },
        "ee40094f6c3344248c52877e1dba89dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed62a1e33a974d1fabd3301ab70ac0bc",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_54c027fdc50e49749c377bf1b5e6b474",
            "value": "‚Äá5.94k/?‚Äá[00:00&lt;00:00,‚Äá140kB/s]"
          }
        },
        "88766b35f21d47bea107022f91a0a750": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fde7cf5ef1cc4795a4b88aed117c2449": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ec485a4cc7c423785be229d03d422fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9bd0d9ad7e444ff4916c2fe3328e1529": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "f3d345f7a5104992ad44812c7f9fae1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ed62a1e33a974d1fabd3301ab70ac0bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54c027fdc50e49749c377bf1b5e6b474": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ed3351658f444a698667be201ed0427": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_baec5f9d64334664bbfac39eb8ff8ce7",
              "IPY_MODEL_56e18f478e9544db981dedea22c6a325",
              "IPY_MODEL_06501b3e310346d0b8ce440071bc3f48"
            ],
            "layout": "IPY_MODEL_f6b7242606e3438f83a17c90a7cc83b4"
          }
        },
        "baec5f9d64334664bbfac39eb8ff8ce7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e9d1c735c014182afa7749d68efb958",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ccd3147f6c5b461ebbdcad0cd1d740f7",
            "value": "Downloading‚Äáextra‚Äámodules:‚Äá"
          }
        },
        "56e18f478e9544db981dedea22c6a325": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17e7946330854e4e89e83ed5e6834151",
            "max": 1554,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ee05a617f1514c05a8e25d6133eccd11",
            "value": 1554
          }
        },
        "06501b3e310346d0b8ce440071bc3f48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d1dcd52b58f4e849988b6b081e1f9a3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1a8d22fbe7d647e0b4e9d109341f43dc",
            "value": "‚Äá4.07k/?‚Äá[00:00&lt;00:00,‚Äá141kB/s]"
          }
        },
        "f6b7242606e3438f83a17c90a7cc83b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e9d1c735c014182afa7749d68efb958": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccd3147f6c5b461ebbdcad0cd1d740f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17e7946330854e4e89e83ed5e6834151": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee05a617f1514c05a8e25d6133eccd11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2d1dcd52b58f4e849988b6b081e1f9a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a8d22fbe7d647e0b4e9d109341f43dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b1a078981a545eabb71718b8e0b5b44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3d84afc5db384936aa19311b115a0143",
              "IPY_MODEL_0a26e2e959b4490ebe5f91a7f16301c8",
              "IPY_MODEL_45f99c55942a4037bdfc7e9e72540bd2"
            ],
            "layout": "IPY_MODEL_48ef7e9c2ee44ef0926fe6e34013abf5"
          }
        },
        "3d84afc5db384936aa19311b115a0143": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f26645541d98445f9b8f2b46169cc902",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_028b501c80484e8eaa458477e2099f39",
            "value": "Downloading‚Äáextra‚Äámodules:‚Äá"
          }
        },
        "0a26e2e959b4490ebe5f91a7f16301c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6740e4e3701047eba0a70ecc431d0ec2",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5ef1f3a355604090a290ab9161529c9e",
            "value": 1
          }
        },
        "45f99c55942a4037bdfc7e9e72540bd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6208cf531ec94b2b8380d57fb6573fdb",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_62a1f563f5b949529a6a775558fe9641",
            "value": "‚Äá3.34k/?‚Äá[00:00&lt;00:00,‚Äá100kB/s]"
          }
        },
        "48ef7e9c2ee44ef0926fe6e34013abf5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f26645541d98445f9b8f2b46169cc902": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "028b501c80484e8eaa458477e2099f39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6740e4e3701047eba0a70ecc431d0ec2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "5ef1f3a355604090a290ab9161529c9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6208cf531ec94b2b8380d57fb6573fdb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62a1f563f5b949529a6a775558fe9641": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8fb705b8c1be4c149dc282766afea7f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_490c3b7c194e4c9987f8d040a96bdf2d",
              "IPY_MODEL_b09ec53898224e72aa7fb2919a26919e",
              "IPY_MODEL_970be341b485499a9fbd931036334b2d"
            ],
            "layout": "IPY_MODEL_71c1b4dbf31c430bbd321c148ef72786"
          }
        },
        "490c3b7c194e4c9987f8d040a96bdf2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_869047f047a6439f84664969e192c5d1",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_00a12b5aa6fa4d71a3bb5939bcd71e5a",
            "value": "Downloading‚Äábuilder‚Äáscript:‚Äá"
          }
        },
        "b09ec53898224e72aa7fb2919a26919e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d806064075e14a90aa95611295017b1e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3e710baaf84844b6a23b0eba44cec1a5",
            "value": 1
          }
        },
        "970be341b485499a9fbd931036334b2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14c7af6dd6e84ace8b78ce74059c7d57",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e225c13b57694958a611fa47425db521",
            "value": "‚Äá7.95k/?‚Äá[00:00&lt;00:00,‚Äá194kB/s]"
          }
        },
        "71c1b4dbf31c430bbd321c148ef72786": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "869047f047a6439f84664969e192c5d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00a12b5aa6fa4d71a3bb5939bcd71e5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d806064075e14a90aa95611295017b1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "3e710baaf84844b6a23b0eba44cec1a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "14c7af6dd6e84ace8b78ce74059c7d57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e225c13b57694958a611fa47425db521": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}